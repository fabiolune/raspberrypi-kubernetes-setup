# Why

There are many resources that can help you setting up a kubernetes cluster on a Raspberry Pi, but many of them only focus on some specific aspects.
The idea of this repo is to try to collect all the aspects of a decent kubernetes setup for a Raspberry Pi, from dedicated considerations on the ARM architecture, to some basic networking aspects of a home made cluster.

# What

This repo contains the bare minimum components to have a kubernetes cluster up & running on my raspberry pi(s).

The minimal setup is based on:
- [k3s](https://k3s.io/): a lightweight kubernetes distribution
- [nginx ingress controller](https://kubernetes.github.io/ingress-nginx/)
- [cert-manager](https://cert-manager.io/) to manage tls certificates generation

The plan is to extend this list to include tools for monitoring, logging and other functionalities.

## Initial setup

To work with kubernetes we will need some cli tools to interact with our clusters.
Some of them are:
- kubectl
- helm

A fantastic way to install different clis is provided by [arkade](https://github.com/alexellis/arkade), a tool created by Alex Ellis.
To install `arkade` simply run:

```console
curl -sLS https://dl.get-arkade.dev | sudo sh
```

When arkade setup is done, to install kubectl and helm you simply need to run:

```console
ark get kubectl
ark get helm
```
> ark is a handy alias of arkade

## Setting up k3s

The easiest way to install k3s is to run the following command:
```console
curl -sfL https://get.k3s.io | sh -
```

This will create your master node, setup a systemctl service, create a kubectl configuration file and some shell scripts that can be used to stop it and/or uninstall it.
Nevertheless, this simple setup also deploys the default ingress controller (traefik), but since I decided to go with nginx, it is possible to change the setup command in:

```console
curl -sfL https://get.k3s.io | sh -s - --no-deploy traefik
```
> full documentation for the setup options of a master node is available [here](https://rancher.com/docs/k3s/latest/en/installation/install-options/server-config/)

Now you can check that the cluster is up and running using kubectl, but first you have to retrieve the config file to connect to the cluster.

This file is automatically generated by the k3s setup and is located in `/etc/rancher/k3s/k3s.yaml`; a simple way to use it is to copy the file in your local `.kube` directory and define the `KUBECONFIG` environment variable:

```console
sudo cp /etc/rancher/k3s/k3s.yaml ${HOME}/.kube/k3s-config
export KUBECONFIG=${HOME}/.kube/k3s-config
```

To verify that your cluster is up and running, now you can run:

```console
$ kubectl get node

NAME           STATUS   ROLES    AGE   VERSION
raspberrypi    Ready    master   12s   v1.19.5+k3s2
```

## Install Nginx Ingress controller

The Nginx ingress controller provides a way to manage the incoming traffic using nginx as a reverse proxy.

To install the ingress controller the best reference to follow is available at the official website[^1], in particular I will follow the helm approach described [here](https://kubernetes.github.io/ingress-nginx/deploy/#using-helm).

The documentation describes some peculiarities of the bare metal setup that I suggest to read [here](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/); the main points are related to:

- having a service for the controller of type __NodePort__ instead of __LoadBalancer__ (usually automatically provided by the cloud provider for managed clusters like AKS, EKS, ...)
- making sure that the nginx controller is only deployed on a single node (the one that will be exposed to the internet traffic through the router port forwarding)

To ensure the second point first of all we need to tag the node in such a way that it will be the selected by the kubernetes scheduler for the nginx controller pods. Assuming a name `raspberrypi` for the node (which coincide with the hostname), then you can execute the following command:

```console
kubectl label nodes raspberrypi external-exposed=true
```

Now we can move on with the ingress controller setup.

The first step is to add the nginx helm repo and update helm:

```console
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```

Now you are ready to install the helm chart with the following command:

```console
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
	--set controller.nodeSelector.external-exposed="true" \
	--set controller.service.type=NodePort \
	--set controller.service.nodePorts.http=30080 \
	--set controller.service.nodePorts.https=30443 \
	--set controller.service.externalTrafficPolicy=Local \
	--set defaultBackend.enabled=true \
	--set defaultBackend.image.repository=k8s.gcr.io/defaultbackend-arm
```

With this set of values the ingress controller will be deployed on the kubernetes cluster, the controller pods will be scheduled on the node labeled with `external-exposed=true`, will be exposed with a service of type NodePort on the ports 30080 (http) and 30443 (https), will preserve the source IP thanks to the `externalTrafficPolicy` and will have a default backend with a dedicated arm image (the default backend image is not multi architecture, a specific tag is required) for all the requests that do not match any ingress definition.

To check the availability of the ingress controller you should see something similar to:

```console
$ kubectl get pod

NAME                                            READY   STATUS    RESTARTS   AGE
ingress-nginx-defaultbackend-6b59ff499f-5dhjx   1/1     Running   0          15s
ingress-nginx-controller-f5b8f5b4-s6q6z         1/1     Running   0          15s
```

Now, since no ingress resources are defined for any backend, every http request (to the node port 30080 as defined in the helm deploy) to the ingress entry-point will return a 404 (except `/healthz`). The request `curl -i http://localhost:30080/whatever`will give something like:

```console
HTTP/1.1 404 Not Found
Date: Thu, 07 Jan 2021 22:43:44 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 21
Connection: keep-alive

default backend - 404
```

The same is also true for https requests on the port 30443 (now we need to accept insecure connections with `-k` because we did not provide any tls certificate) of the form `curl -ik https://localhost:30443/whatever`

## Install cert manager

The cert manager setup is done directly following the approach suggested in the official [documentation](https://cert-manager.io/docs/).

First we need a dedicated namespace:

```console
kubectl create ns cert-manager
```

Then we need to add the cert manager helm repository:

```console
helm repo add jetstack https://charts.jetstack.io
helm repo update
```

Finally we can deploy it:

```console
helm upgrade --install cert-manager jetstack/cert-manager \
	--namespace cert-manager \
	--version v1.1.0 \
	--set installCRDs=true
```

To be able to generate ACME certificates with _Let's Encrypt_, we need to have a ClusterIssuer (or issuer, the difference is that an Issuer is bound to a namespace) resource on the cluster:

```console
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # The ACME server URL
    server: https://acme-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: fabiolune@gmail.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx
```









---

[^1]:https://kubernetes.github.io/ingress-nginx/deploy